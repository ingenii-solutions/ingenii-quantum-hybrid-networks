{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8382180-2968-44ab-82e1-55bfa0809fdd",
   "metadata": {},
   "source": [
    "## Quantum Hadamard edge detector\n",
    "\n",
    "In this notebook we provide an implementation of the Quantum Hadamard Edge Detection (QHED) algorithm [1], for 2 images and 3D volumes.  We provide an implementation on Qiskit, following the tutorial [2], which is meant to run on real hardware or fake simulators. We also provide an implementation with pytorch, which is optimized to work faster in quantum simulation (CPUs and GPUs).\n",
    "\n",
    "\n",
    "[1]  Yao, Xi-Wei, et al. \"Quantum image processing and its application to edge detection: theory and experiment.\" Physical Review X 7.3 (2017): 031041.\n",
    "\n",
    "[2] Quantum Edge Detection - QHED Algorithm on Small and Large Images https://qiskit.org/textbook/ch-applications/quantum-edge-detection.html#Quantum-Hadamard-Edge-Detection-(QHED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa5461-59ab-4385-85de-bd028f457fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch import device, tensor, zeros\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data.data_reader import Dataset_MLHDF\n",
    "from data.img_util import GaussianFilter, Voxelizer3D\n",
    "from ingenii_quantum.hybrid_networks.edge_detection import EdgeDetector2D, EdgeDetector3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96b11d-08df-4aa9-a4d2-8575e936994d",
   "metadata": {},
   "source": [
    "## 2D example\n",
    "\n",
    "We first provide the code to run a 2D image with Qiskit. This code is meant to be run in a real quantum computer or a fake provider. We use the same images as in the tutorial [2]. Since the image is quite large ($256 \\times 256$), we can not process the whole image with the current hardware (or quantum simulation). For this reason, we split the image into ($4\\times 4$) blocks, and apply the edge algorithm to each block. Let's start by visualizing the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64325519-0505-4925-820e-4cc9249f5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_size = 256       # Original image-width\n",
    "image_crop_size = 32   # Width of each part of image for processing\n",
    "\n",
    "# Load the image from filesystem\n",
    "image_raw = np.array(Image.open('images/edge-detection.png'))\n",
    "print('Raw Image info:', image_raw.shape)\n",
    "print('Raw Image datatype:', image_raw.dtype)\n",
    "\n",
    "# Convert the RBG component of the image to B&W image, as a numpy (uint8) array\n",
    "image = np.array([\n",
    "    [\n",
    "        image_raw[i][j][0] / 255\n",
    "        for j in range(image_size)\n",
    "    ]\n",
    "    for i in range(image_size)\n",
    "])\n",
    "            \n",
    "print('Image shape (numpy array):', image.shape)\n",
    "\n",
    "\n",
    "# Display the image\n",
    "plt.title('Big Image')\n",
    "plt.xticks(range(0, image.shape[0]+1, 32))\n",
    "plt.yticks(range(0, image.shape[1]+1, 32))\n",
    "plt.imshow(image, extent=[0, image.shape[0], image.shape[1], 0], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a16e1c-5626-49d0-b4d7-e9c64e277772",
   "metadata": {},
   "source": [
    "### Run with Qiskit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9de780-f5a0-48fd-ab74-0bca7c35c779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You could try running it with a fake backend, but it takes a lot of time...\n",
    "#from qiskit.providers.fake_provider import FakeMelbourne\n",
    "#backend =  FakeMelbourne()\n",
    "data =  np.array(image).reshape(1, image_size, image_size)\n",
    "edge_detector = EdgeDetector2D(size=4, shots=2**10, backend='aer_simulator')\n",
    "reverted_image = edge_detector.run(\n",
    "    data, tol=1e-5, reduce=False, verbose=True).reshape(image_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bc371-bf9b-4ea0-adc5-7dde4cb2e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plt.title('Big Image')\n",
    "plt.xticks(range(0, reverted_image.shape[0]+1, 32))\n",
    "plt.yticks(range(0, reverted_image.shape[1]+1, 32))\n",
    "plt.imshow(reverted_image, extent=[0, reverted_image.shape[0], reverted_image.shape[1], 0], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90013d49-a814-4cfe-ba01-3d47005b69c9",
   "metadata": {},
   "source": [
    "## Run with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af9d00-f6ef-4f29-affd-7fe8ec3ff7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tensor(image).reshape(1, image_size, image_size)\n",
    "edge_detector = EdgeDetector2D(size=4, backend='torch')\n",
    "reverted_image = edge_detector.run(\n",
    "    data, tol=1e-2, reduce=False, verbose=True).reshape(image_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57b6c2-d882-4e7b-9bb5-43853e5bb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plt.title('Big Image')\n",
    "plt.xticks(range(0, reverted_image.shape[0]+1, 32))\n",
    "plt.yticks(range(0, reverted_image.shape[1]+1, 32))\n",
    "plt.imshow(reverted_image, extent=[0, reverted_image.shape[0], reverted_image.shape[1], 0], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0a375-bff5-429d-9c2f-d23020d683a8",
   "metadata": {},
   "source": [
    "The edge detection algorithm works! Notice that running the algorithm with the pytorch backend is much faster than running it with quantum simulation in the Qiskit backend. Also, the results using the qiskit backend depend on the number of shots of the experimet, that should be large enough to produce good results. \n",
    "\n",
    "Another parameter of the algorithm is the tolerance *tol*. It measures how big does the gradient need to be so that it is considered an edge. This value should be tuned for different datasets, so it should be set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777f178-2005-4097-87d7-12ffc89814a7",
   "metadata": {},
   "source": [
    "## Application to 3D data\n",
    "\n",
    "Now we show an example of a 3D volume, processed from the PDBBind dataset for protein-ligand prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7b0bb-b4e4-4c21-a083-94a8a63948f3",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We begin by loading some samples of the data and visualizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14231b64-5535-4bc5-ab79-9ad139e47388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset_MLHDF(\n",
    "    'data/pdbbind2016_core_test.hdf', 1, 'data/core_test_3dnn.csv',\n",
    "    is_crystal=True, rmsd_weight=0, rmsd_thres=20\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, worker_init_fn=None)\n",
    "voxelizer = Voxelizer3D(use_cuda=False, verbose=True)\n",
    "gaussian_filter = GaussianFilter(dim=3, channels=19, kernel_size=11, sigma=1, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbf296-b121-4280-aa0f-d7e69acb86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_batch = zeros((1,19,48,48,48)).float().to(device(\"cpu\"))\n",
    "for batch_ind, batch in enumerate(dataloader):\n",
    "    x_batch, y_batch = batch\n",
    "       # voxelize into 3d volume\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        xyz, feat = x_batch[i,:,:3], x_batch[i,:,3:]\n",
    "        vol_batch[i,:,:,:,:] = voxelizer(xyz, feat)\n",
    "    vol_batch_gaus = gaussian_filter(vol_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684535f-b28e-4fae-92ce-d88c229c8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show an example data sample with and without gaussian blur\n",
    "example_data_sample_gaussian_blur = vol_batch_gaus[0][1].numpy()\n",
    "example_data_sample = vol_batch[0][1].numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax1.voxels(example_data_sample, facecolors='yellow', alpha=0.4, edgecolor='k')\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Data without gaussian blur')\n",
    "\n",
    "colors = plt.cm.plasma(example_data_sample_gaussian_blur)\n",
    "ax2.voxels(example_data_sample_gaussian_blur, facecolors=colors, alpha=0.2, edgecolor='k')\n",
    "\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "ax2.set_title('Data with gaussian blur')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788af76-d1b4-4749-ab16-2b885c3c2497",
   "metadata": {},
   "source": [
    "## Run with pytorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ff81c-5310-46fb-ac30-17c125275d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_detector = EdgeDetector3D(size=16, backend='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa1d06-fbba-4247-9592-07ac661890c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vol_batch_gaus\n",
    "data_processed = edge_detector.run(data, num_filters=1, tol=3e-3, reduce=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce57c2-095d-4706-b1de-bd7b20ecf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = data[0, 10].numpy()\n",
    "d = data_processed[0, 10].numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "colors = plt.cm.plasma(d)\n",
    "ax1.voxels(d, facecolors=colors, alpha=0.4, edgecolor='k')\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "\n",
    "colors = plt.cm.plasma(d2)\n",
    "ax2.voxels(d2, facecolors=colors, alpha=0.4, edgecolor='k')\n",
    "\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e806d-fb8c-442f-988b-91b8b94d66dd",
   "metadata": {},
   "source": [
    "## Run with Qiskit backend\n",
    "\n",
    "Now we run the code with Qiskit backend in quantum simulation. We see that the execution time is much longer than running the algorithm with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529abaf-27d1-437c-903f-76712271f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_detector = EdgeDetector3D(size=4, shots=2**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f69271-7230-43b0-833d-3b2f863bd117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = vol_batch_gaus\n",
    "data_processed = edge_detector.run(data.numpy().copy(), num_filters=3, tol=3e-3, reduce=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb560e5-3c56-468a-bf04-76001e0549f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = data[0, 10].numpy()\n",
    "d = data_processed[0, 10]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(1 ,2, 2, projection='3d')\n",
    "colors = plt.cm.plasma(d)\n",
    "ax1.voxels(d, facecolors=colors, alpha=0.4, edgecolor='k')\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "\n",
    "colors = plt.cm.plasma(d2)\n",
    "ax2.voxels(d2, facecolors=colors, alpha=0.4, edgecolor='k')\n",
    "\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448feb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
